{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from scipy.stats import ttest_ind, ttest_ind_from_stats\n",
    "\n",
    "data = pd.read_csv('../Research Project/namesR.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.Series(data.user).unique()\n",
    "items = pd.Series(data.item).unique()\n",
    "items = np.sort(items)\n",
    "\n",
    "openness = []\n",
    "conscientiousness = []\n",
    "extraversion = []\n",
    "agreeableness = []\n",
    "neuroticism = []\n",
    "\n",
    "for user in users:\n",
    "    dfuser = data.loc[data['user'] == user]\n",
    "    openness.append(dfuser.iloc[0]['openness'])\n",
    "    conscientiousness.append(dfuser.iloc[0]['conscientiousness'])\n",
    "    extraversion.append(dfuser.iloc[0]['extraversion'])\n",
    "    agreeableness.append(dfuser.iloc[0]['agreeableness'])\n",
    "    neuroticism.append(dfuser.iloc[0]['neuroticism'])\n",
    "\n",
    "df_user_personality = pd.DataFrame({ 'user' : users,\n",
    "                                     'openness' : openness,\n",
    "                                     'conscientiousness' : conscientiousness,\n",
    "                                     'extraversion' : extraversion,\n",
    "                                     'agreeableness' : agreeableness,\n",
    "                                     'neuroticism' : neuroticism })\n",
    "\n",
    "\n",
    "df_user_personality.head()\n",
    "col_names =  list(data)\n",
    "users = df_user_personality.index + 1\n",
    "sample0 = pd.DataFrame(columns = col_names)\n",
    "sample1 = pd.DataFrame(columns = col_names)\n",
    "sample2 = pd.DataFrame(columns = col_names)\n",
    "sample3 = pd.DataFrame(columns = col_names)\n",
    "sample4 = pd.DataFrame(columns = col_names)\n",
    "\n",
    "for user in users:\n",
    "    df_user = data.loc[data['user'] == user]\n",
    "    s0 = np.array_split(df_user, 5)[0]\n",
    "    s1 = np.array_split(df_user, 5)[1]\n",
    "    s2 = np.array_split(df_user, 5)[2]\n",
    "    s3 = np.array_split(df_user, 5)[3]\n",
    "    s4 = np.array_split(df_user, 5)[4]\n",
    "    sample0 = pd.concat([sample0, s0])\n",
    "    sample1 = pd.concat([sample1, s1])\n",
    "    sample2 = pd.concat([sample2, s2])\n",
    "    sample3 = pd.concat([sample3, s3])\n",
    "    sample4 = pd.concat([sample4, s4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_noP(R, P, Q, K, rating_mean, user_bias, item_bias, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - (rating_mean + item_bias[j] + user_bias[i] + np.dot(P[i,:],Q[:,j]))\n",
    "                    item_bias[j] = item_bias[j] + alpha * (eij - beta * item_bias[j]) \n",
    "                    user_bias[i] = user_bias[i] + alpha * (eij - beta * user_bias[i]) \n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (eij * P[i][k] - beta * Q[k][j])\n",
    "    return P, Q.T\n",
    "\n",
    "def metrics(rated_items_test, predictions_noP):\n",
    "    \n",
    "    precisionV = 0\n",
    "    totalP = 0\n",
    "\n",
    "    recallV = 0\n",
    "    totalR = 0\n",
    "    \n",
    "    ndcgV = 0\n",
    "    totalN = len(users)\n",
    "\n",
    "    nrec = 10\n",
    "\n",
    "    for user in users:\n",
    "\n",
    "        itemsR = rated_items_test.T.nlargest(nrec, user)[0:10][user].index.tolist()\n",
    "        itemsP = predictions_noP.T.nlargest(10, user)[0:10][user].index.tolist()\n",
    "        \n",
    "        score = []\n",
    "        #print(ratingsP)\n",
    "        for item in itemsP:\n",
    "            meanrating = np.mean(predictions_noP[item])\n",
    "            score.append(meanrating)\n",
    "        #print(score)\n",
    "        a = np.asfarray(score)\n",
    "        b = np.sort(a)\n",
    "        b = np.asfarray(b[::-1])\n",
    "        #print(b)\n",
    "        t =  a[0] + np.sum(a[1:] / np.log2(np.arange(2, a.size + 1)))\n",
    "        t2 =  b[0] + np.sum(b[1:] / np.log2(np.arange(2, b.size + 1)))\n",
    "        \n",
    "        #print(t/t2)\n",
    "        \n",
    "        ndcgV = ndcgV + (t/t2)\n",
    "        \n",
    "        for i in range(len(itemsP)):\n",
    "            itemsP[i] = int(itemsP[i]/100)\n",
    "    \n",
    "        totalP = totalP + len(itemsR)\n",
    "    \n",
    "        df = predictions_noP.copy().T\n",
    "    \n",
    "        ratingsR = rated_items_test.T.nlargest(nrec, user)[0:10][user].tolist()\n",
    "        ratingsP = df.nlargest(10, user)[0:10][user].tolist()\n",
    "    \n",
    "        for item in itemsR:\n",
    "            if item in itemsP:\n",
    "                precisionV = precisionV + 1\n",
    "            if(ratingsR[itemsR.index(item)]>3):\n",
    "                totalR = totalR + 1\n",
    "                if(item in itemsP):\n",
    "                    if(ratingsP [itemsP.index(item)]>3.5):\n",
    "                        recallV = recallV+1 \n",
    "                        \n",
    "    T = np.array(rated_items_test.astype(float))\n",
    "    R = np.array(predictions_noP.astype(float))                    \n",
    "    error = 0\n",
    "    totalE = 0\n",
    "    for i in range(len(T)):\n",
    "        for j in range(len(T[i])):\n",
    "            if(T[i][j]>0):\n",
    "                totalE = totalE + 1\n",
    "                sqe = (T[i][j] - R[i][j])\n",
    "                error = sqe*sqe\n",
    "    \n",
    "    return (precisionV/totalP)*100, (recallV/totalR)*100, math.sqrt(error/totalE), ndcgV/totalN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION\n",
      "0\n",
      "9.422492401215806\n",
      "42.857142857142854\n",
      "0.1374461003222707\n",
      "0.999270818178852\n",
      "ITERATION\n",
      "1\n",
      "16.666666666666664\n",
      "16.627634660421545\n",
      "0.028635271136281094\n",
      "0.9995172228013469\n",
      "ITERATION\n",
      "2\n",
      "16.05839416058394\n",
      "24.034334763948497\n",
      "0.08525999942018431\n",
      "0.999010664345069\n",
      "ITERATION\n",
      "3\n",
      "27.34584450402145\n",
      "31.759656652360512\n",
      "0.08433656918324954\n",
      "0.999037439679321\n",
      "ITERATION\n",
      "4\n",
      "20.689655172413794\n",
      "30.21978021978022\n",
      "0.06802775033360099\n",
      "0.9991868463633051\n"
     ]
    }
   ],
   "source": [
    "precision = np.empty([1,5])\n",
    "recall = np.empty([1,5])\n",
    "ndcg = np.empty([1,5])\n",
    "error = np.empty([1,5])\n",
    "\n",
    "for iteration in range(5):\n",
    "    print('ITERATION')\n",
    "    print(iteration)\n",
    "    if(iteration == 0):\n",
    "        train = pd.concat([sample0, sample1, sample2, sample3])\n",
    "        test = sample4\n",
    "    elif(iteration == 1):\n",
    "        train = pd.concat([sample1, sample2, sample3, sample4])\n",
    "        test = sample0\n",
    "    elif(iteration == 2):\n",
    "        train = pd.concat([sample2, sample3, sample4, sample0])\n",
    "        test = sample1        \n",
    "    elif(iteration == 3):\n",
    "        train = pd.concat([sample3, sample4, sample0, sample1])\n",
    "        test = sample2\n",
    "    else:\n",
    "        train = pd.concat([sample4, sample0, sample1, sample2])\n",
    "        test = sample3\n",
    "    itemsOG = train['item'].copy()\n",
    "\n",
    "    items = pd.Series(data.item).unique()\n",
    "    items = np.sort(items)\n",
    "    Npersonalities = len(df_user_personality.columns)-1\n",
    "    MaxV = max(df_user_personality.loc[:, df_user_personality.columns != 'user'].max())\n",
    "    totalclasses = MaxV*Npersonalities\n",
    "    personality = ['agreeableness','conscientiousness','extraversion','neuroticism','openness']\n",
    "\n",
    "    for item in items:\n",
    "        df_item = train.loc[train['item'] == item]\n",
    "        ratings = df_item['rating']\n",
    "        tValue = []\n",
    "        pValue = []\n",
    "        for pers in personality:\n",
    "            for i in range(1, MaxV+1):\n",
    "                #print(pers + ': ' + str(i))\n",
    "                rating_T = []\n",
    "                rating_F = []\n",
    "                position = df_item.index\n",
    "                j = 0\n",
    "                for rating in ratings:\n",
    "                    if(df_item[pers][position[j]] == i):\n",
    "                        rating_T.append(rating)\n",
    "                    else:\n",
    "                        rating_F.append(rating)\n",
    "                    j = j + 1\n",
    "                #print('/////')\n",
    "                #print(rating_T)\n",
    "                #print(len(rating_T))\n",
    "                #print(rating_F)\n",
    "                #print(len(rating_F))\n",
    "                #print('///')\n",
    "                if (len(rating_T) > 1 and len(rating_F) > 1):\n",
    "                    t, p = ttest_ind(rating_T, rating_F, equal_var=False)\n",
    "                    #print(t)\n",
    "                    #print(p)\n",
    "                    tValue.append(t)\n",
    "                    pValue.append(p)\n",
    "        if(len(tValue) > 0 and len(pValue) > 0):            \n",
    "            maxT = -1000\n",
    "            indexMaxT = 0\n",
    "            for i in range(len(pValue)):\n",
    "                if (pValue[i]<0.05) & (tValue[i]>maxT):\n",
    "                    maxT = tValue[i]\n",
    "                    indexMaxT = i\n",
    "        \n",
    "        personalityI = int(round(indexMaxT/7+0.51))-1\n",
    "        personalityV = indexMaxT+1-(personalityI)*7\n",
    "        \n",
    "        \n",
    "        #print(personality[personalityI] + \" \" + str(personalityV))\n",
    "        \n",
    "        for indexIN in df_item.index:\n",
    "            if(df_item[personality[personalityI]][indexIN] == personalityV):\n",
    "                #print('Hello')\n",
    "                train['item'][indexIN] = int(str(item) + \"01\")\n",
    "            else:\n",
    "                #print('Hi')\n",
    "                train['item'][indexIN] = int(str(item) + \"02\")\n",
    "    \n",
    "        else:\n",
    "            continue\n",
    "    rated_items_train = pd.DataFrame(index=users, columns=np.sort(pd.Series(train.item).unique()), dtype='float')\n",
    "\n",
    "    rated_items_test = pd.DataFrame(index=users, columns=items, dtype='float')\n",
    "\n",
    "    for user in users:\n",
    "        df_user_rat = train.loc[train['user'] == user]\n",
    "        items_rat = df_user_rat['item'].tolist()\n",
    "        rating = df_user_rat['rating'].tolist()\n",
    "        \n",
    "        df_user_rat_t = test.loc[test['user'] == user]\n",
    "        items_rat_t = df_user_rat_t['item'].tolist()\n",
    "        rating_t = df_user_rat_t['rating'].tolist()\n",
    "        \n",
    "        i = 0\n",
    "        for item in items_rat:\n",
    "            rated_items_train.loc[user][item] = rating[i]\n",
    "            i = i + 1\n",
    "        i = 0\n",
    "        for item in items_rat_t:\n",
    "            rated_items_test.loc[user][item] = rating_t[i]\n",
    "            i = i + 1\n",
    "    R = np.array(rated_items_train.astype(float))\n",
    "    rating_mean = train['rating'].mean()\n",
    "    N = len(R)\n",
    "    M = len(R[0])\n",
    "    K = 10\n",
    "    P = np.random.normal(scale= 1/K, size=(N,K))\n",
    "    Q = np.random.normal(scale= 1/K, size=(M,K))\n",
    "    user_bias = np.random.normal(scale= 1/K, size=(N,1))\n",
    "    item_bias = np.random.normal(scale= 1/K, size=(M,1))\n",
    "\n",
    "    nP, nQ = matrix_factorization_noP(R, P, Q, K, rating_mean, user_bias, item_bias, steps = 120, alpha= 0.001, beta= 0.002)\n",
    "\n",
    "    for i in range(len(R)):\n",
    "        for j in range(len(R[i])):\n",
    "            R[i][j] = rating_mean + user_bias[i] + item_bias[j] + np.dot(P[i,:],Q.T[:,j])\n",
    "            \n",
    "    predictions_noP = pd.DataFrame(index=users, columns=np.sort(pd.Series(train.item).unique()), data = R)\n",
    "    p, r, e, n = metrics(rated_items_test, predictions_noP)\n",
    "    #precision[0][iteration] = p\n",
    "    #recall[0][iteration] = r\n",
    "    precision[0][iteration] = p\n",
    "    recall[0][iteration] = r\n",
    "    ndcg[0][iteration] = ndcg\n",
    "    error[0][iteration] = e\n",
    "    print(p)\n",
    "    print(r)\n",
    "    print(e)\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Metrics already recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.0001, Beta = 0.002, Nsteps = 100\n",
      "16.315385230702013\n",
      "23.59269837318103\n",
      "0.08411096084490563\n",
      "\n",
      "Alpha = 0.00001, Beta = 0.002, Nsteps = 120\n",
      "14.562691036596817\n",
      "7.939736334867552\n",
      "0.0872954952128341\n",
      "\n",
      "Alpha = 0.001, Beta = 0.02, Nsteps = 120\n",
      "16.766594798154813\n",
      "28.672728805002073\n",
      "0.07763716448384479\n",
      "\n",
      "Alpha = 0.01, Beta = 0.02, Nsteps = 120\n",
      "17.637293830785914\n",
      "29.738669829431807\n",
      "0.05959073556053657\n"
     ]
    }
   ],
   "source": [
    "precision_noP1 = [12.158054711246201, 19.017094017094017, 15.328467153284672, 20.37533512064343, 16.954022988505745]\n",
    "recall_noP1 = [50.0, 20.140515222482435, 22.746781115879827, 25.75107296137339, 24.725274725274726]\n",
    "e_noP1 = [0.12819255254417394, 0.03353121641931519, 0.08204314836723474, 0.07809071329681223, 0.06632819179168789]\n",
    "\n",
    "precision_noP2 = [10.638297872340425, 18.162393162393162, 14.355231143552311, 21.179624664879356, 17.24137931034483]\n",
    "recall_noP2 = [46.42857142857143, 6.0889929742388755, 19.742489270386265, 23.17596566523605, 22.52747252747253]\n",
    "e_noP2 = [0.13518802561971324, 0.04296571741251353, 0.08172812043111354, 0.08507506175105764, 0.07559787901013024]\n",
    "\n",
    "precision_noP3 = [19.45288753799392, 9.18803418803419, 12.895377128953772, 16.621983914209114, 14.655172413793101]\n",
    "recall_noP3 = [0.0, 0.468384074941452, 13.304721030042918, 9.44206008583691, 16.483516483516482]\n",
    "e_noP3 = [0.1551877742566284, 0.043044314275938536, 0.09305661886920405, 0.07178072152306228, 0.0734080471393372]\n",
    "\n",
    "precision_noP4 = [10.94224924012158, 21.367521367521366, 13.62530413625304, 23.86058981233244, 18.39080459770115]\n",
    "recall_noP4 = [50.0, 22.482435597189696, 18.4549356223176, 29.184549356223176, 28.57142857142857]\n",
    "e_noP4 = [0.0748992849478179, 0.015957687848008884, 0.04948561647266341, 0.07065609635756304, 0.08695499217662961]\n",
    "\n",
    "print('Alpha = 0.0001, Beta = 0.002, Nsteps = 100')\n",
    "print(np.mean(precision_noP2))\n",
    "print(np.mean(recall_noP2))\n",
    "print(np.mean(e_noP2))\n",
    "print()\n",
    "print('Alpha = 0.00001, Beta = 0.002, Nsteps = 120')\n",
    "print(np.mean(precision_noP3))\n",
    "print(np.mean(recall_noP3))\n",
    "print(np.mean(e_noP3))\n",
    "print()\n",
    "print('Alpha = 0.001, Beta = 0.02, Nsteps = 120')\n",
    "print(np.mean(precision_noP1))\n",
    "print(np.mean(recall_noP1))\n",
    "print(np.mean(e_noP1))\n",
    "print()\n",
    "print('Alpha = 0.01, Beta = 0.02, Nsteps = 120')\n",
    "print(np.mean(precision_noP4))\n",
    "print(np.mean(recall_noP4))\n",
    "print(np.mean(e_noP4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
